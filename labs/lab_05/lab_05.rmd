---
title: "lab_05"
author: "Mitchell Hang"
date: "2023-03-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## You will need

- Tabula

## Load libraries and establish settings

```{r}
# Turn off scientific notation
options(scipen=999)

# Load the tidyverse.
library(tidyverse)
library(lubridate)
library(janitor)
```

## Get Our PDF

We'll be working with the [911 overdose calls from Baltimore County](https://drive.google.com/file/d/1qkYuojGF_6WKFr5aNQxmewDzcKyOiJFr/view?usp=share_link). You'll want to download it to a place you'll remember (like your Downloads folder, or the labs folder in your repository). The goal is to extract the tables within it, export that to a CSV file, load it into RStudio and ask some questions.

## Extract Data from PDF Using Tabula

Start Tabula, then go to http://127.0.0.1:8080/ in your browser. Click the "Browse" button and find the PDF file and click "open", and then click the "Import button" in Tabula. This will take a few seconds or longer.

This PDF has a single table spread over multiple pages to extract. We're going to make a single dataframe from this table, exporting it to a CSV file that you will load into R. In Tabula, highlight the table and click the "Preview & Export Extracted Data" button. You may want to play with including or excluding the column headers - YOU SHOULD HAVE FIVE COLUMNS OF DATA.

Save the CSV (it should be called `tabula-Baltimore County; Carey, Samantha log OD.csv` by default) to your lab_05/data folder.

From there, you will need to read in the data, and add or fix headers if necessary. You can choose to include the headers from the PDF in your exported CSV files OR to exclude them and add them when importing. `read_csv` allows us to do this ([and more](https://readr.tidyverse.org/reference/read_delim.html)).

## Load and clean up the data in R

You will need to read in and clean up the data so that it can be used for analysis. By "clean" I mean the column headers should not contain spaces and they should have meaningful names, not "x1" or something similar. How you do that is up to you, but you can use select() with or without the minus sign to include or exclude certain columns. You also can use the `rename` function to, well, rename columns. Importantly, you'll need to ensure that any columns containing a date actually have a date datatype. Our friend `lubridate` can help with this.

```{r}
calls_baltimore_county <- read_csv("data/tabula-Baltimore County; Carey, Samantha log OD.csv", col_names = FALSE) %>% 
  clean_names() %>%
  rename(date = x1, time = x2, case_number = x3, event_type = x4, location = x5) %>%
  mutate(date=mdy(date))

calls_baltimore_county 

```

## Answer questions

Q1. Write code to generate the number of calls that occurred on each date. Which date in 2022 had the most overdose calls, and how many? Look at the total number of rows in your result and explore the range of dates - based on your result, do you believe there are any days with no overdose calls at all? Explain why or why not.

A1. The dates in 2022 which had the most overdose calls in Baltimore County was July 14th and October 4th, with 23 calls each. Based on what I did, I found 329 dates with 911 calls pertaining to overdoses, as the dataset begins on February 6, 2022. If we subtract it from December 31, 2022 to find the number of days, we find that there are 329 days between the two dates, so there doesn't appear to be a date in 2022 without any 911 calls pertaining to overdoses. 

```{r}
calls_baltimore_county %>%
  filter(between(date, as.Date('2022-01-01'), as.Date('2022-12-31'))) %>%
  group_by(date) %>%
  summarize(total_calls = n()) %>%
  arrange(desc(total_calls))
```

Q2. You want to understand if there's a pattern in the day of the week that overdose calls are made. Add a column to your dataframe that displays what day of the week each date represents. You should search for how to do that using lubridate. Then write code to calculate the number of calls for each day of the week, and add a column to that result that calculates the percentage of all calls that occurred on each day of the week (so you want a dataframe with the day of the week, total number of calls and the percentage of calls on that day out of the total number of all calls). Describe your findings to me.

A2. Saturday was the day with the most 911 calls pertaining to overdoses -- with 638 calls -- and it accounted for 15.5% of all overdose calls made at the time. On the other hand, Thursday was the day with the least 911 calls pertaining to overdoses -- with 526 calls -- and it accounted for 12.8% of all overdose calls made at the time. Compared to an even distribution of 100%/7 ~ 14.3%, these are very interesting results, as they suggest that the data is more lop-sided where people make overdose calls during the weekend compared to a weekday.

```{r}
calls_baltimore_county %>%
  mutate(day_of_week=wday(date, label=TRUE)) %>%
  group_by(day_of_week) %>%
  summarize(total_calls = n()) %>%
  arrange(desc(total_calls)) %>%
  mutate(percentage=(total_calls/4112))
```

Q3. Now let's look at locations. Which ones have the most calls? How would you describe them (feel free to search for more information on them)? Is there anything about the structure of the original data that might make you less confident in the counts by location or date?

A3. The location(s) which has the most calls is 4540 Silver Spring Road (in Perry Hall), which -- upon me looking it up -- appears to be an intersection for a street. The second and third highest (they were tied with the same number) appear to be addresses for two precincts of the Baltimore County Police Department. Only the following address, 330 Leeanne Road, makes more sense at it appears to represent a house in a neighborhood. The data makes me feel less confident in terms of how their locations are written, as some of the addresses have a form of "UPDATED LOCATION" in their name, and it makes it harder to conglomerate all of the calls made at said location without having to use separate software like OpenRefine. In general, the top three locations are very odd for someone to report a drug overdose, as I would expect an intersection to be more closely associated with car accidents/crashes.

```{r}
calls_baltimore_county %>%
  group_by(location) %>%
  summarize(total_calls = n()) %>%
  arrange(desc(total_calls))
```

Q4. What's the best story idea or question you've seen as a result of the work you've done in this lab?

A4. I am curious about the overdose calls made at the Baltimore County Police Department precincts, as it could be instances of someone reporting an overdose outside of the building, or a cop accidentally ingesting too much of a drug while at work. Otherwise, I don't know how else I could make an interesting story out of this pretty confusing dataset, especially with that top address not making too much sense.
